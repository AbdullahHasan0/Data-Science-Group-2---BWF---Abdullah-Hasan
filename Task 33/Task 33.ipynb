{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center><font color = '#642AAE'>Neural Networks Basics (Perceptron, Activation Functions) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### What are Neural Networks ??\n",
    "\n",
    "\n",
    "Neural networks are computational models inspired by the structure and function of the human brain. They consist of interconnected nodes, or neurons, that process input data to produce an output. The main components of neural networks include:\n",
    "\n",
    "1. Input Layer: Receives the input data and passes it to the hidden layers.\n",
    "\n",
    "2. Hidden Layers: Intermediate layers that process the input data and generate intermediate representations. They can have multiple neurons and can include activation functions to introduce non-linearity and complex patterns.\n",
    "\n",
    "3. Output Layer: Processes the generated intermediate representations and produces the final output.\n",
    "\n",
    "Neural networks are designed to learn and adapt to complex patterns in data, making them powerful tools for various applications such as image recognition, natural language processing, and recommendation systems.\n",
    "\n",
    "\n",
    "<img src = 'NN.png' alt = \"Neural Networks\">\n",
    "\n",
    "\n",
    "#### What is Perceptron ?\n",
    "\n",
    "A perceptron is a simple neural network model that consists of a single layer of neurons. It is a linear classifier, meaning it can only classify data into two classes (e.g., binary classification). The perceptron is based on the McCulloch-Pitts neuron model, which was introduced by George W. McCulloch and Walter Pitts in 1943.\n",
    "\n",
    "The perceptron algorithm is an iterative learning algorithm used to train a linear classifier. It follows these steps:\n",
    "\n",
    "1. Initialize the weights and bias randomly.\n",
    "\n",
    "2. For each training example:\n",
    "\n",
    "   a. Calculate the predicted output using the current weights and bias.\n",
    "\n",
    "   b. Compare the predicted output with the actual output.\n",
    "\n",
    "   c. Update the weights and bias based on the difference between the predicted output and the actual output.\n",
    "\n",
    "   d. Repeat steps a-c until the desired accuracy is achieved or a maximum number of iterations is reached.\n",
    "\n",
    "   e. After training, the perceptron can be used to make predictions on new input data.\n",
    "\n",
    "   f. The perceptron can also be trained using gradient descent, which updates the weights and bias iteratively to minimize the error between the predicted output and the actual output.\n",
    "\n",
    "   g. The perceptron can be used for binary classification problems, where the output is either 0 or 1.\n",
    "\n",
    "<img src = 'perceptron.jpg' alt = 'Perceptron'>\n",
    "\n",
    "#### Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions are used to introduce non-linearity and complex patterns into the neural network. There are various activation functions available, such as:\n",
    "\n",
    "1. Sigmoid Function: S-shaped curve that outputs values between 0 and 1. The sigmoid function is widely used in binary classification problems.\n",
    "\n",
    "2. Hyperbolic Tangent (tanh) Function: S-shaped curve that outputs values between -1 and 1. The tanh function is similar to the sigmoid function but has a better gradient for training neural networks.\n",
    "\n",
    "3. Rectified Linear Unit (ReLU) Function: Outputs the input value if it is positive, or 0 otherwise. The ReLU function is widely used in neural networks due to its simplicity and efficiency.\n",
    "\n",
    "4. Leaky ReLU Function: Outputs a small negative value (e.g., 0.01) for negative inputs, and the input value for positive inputs. The leaky ReLU function helps mitigate the vanishing gradient problem in neural networks.\n",
    "\n",
    "5. Exponential Linear Unit (ELU) Function: Outputs the input value if it is positive, or an exponential of the input value minus 1 for negative inputs. The ELU function has a better initialization and stability compared to ReLU functions.\n",
    "\n",
    "6. Parametric ReLU Function: Outputs a configurable parameter (e.g., alpha) times the input value if it is positive, or 0 otherwise. The Parametric ReLU function can help improve the performance of neural networks by allowing the network to learn different activation functions for different parts of the input.\n",
    "\n",
    "<img src = 'activationFunction.png' alt='Activation Functions'>\n",
    "\n",
    "By using appropriate activation functions, neural networks can model complex patterns and achieve better performance in various applications, such as image recognition, natural language processing, and recommendation systems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = center> <font color ='#EEFF32'>Implementing Neural Network For Regression Problem From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Initializing The Weights And Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1 matrix will be initialized with random weights and of the shape of hidden_layer_size * input_layer_size and then scalling with 0.01 \n",
    "\n",
    "W2 matrix will be initialized with random weights and of the shape of hidden_layer_size * output_layer_size and then scalling with 0.01 \n",
    "\n",
    "b1 matrix initialized with 0 and of the shape (hidden_layer_size,1)\n",
    "\n",
    "b2 matrix initialized with 0 and of the shape (output_layer_size,1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return(1/(1 + np.exp(-x)))\n",
    "\n",
    "def parameter_initialization(input_layer_size,hidden_layer_size,output_layer_size):\n",
    "    np.random.seed(10)\n",
    "\n",
    "    W1 = np.random.randn(hidden_layer_size,input_layer_size)*0.01\n",
    "    b1 = np.zeros((hidden_layer_size,1))\n",
    "    W2 = np.random.randn(output_layer_size,hidden_layer_size)*0.01\n",
    "    b2 = np.zeros((output_layer_size,1))\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematics\n",
    "\n",
    "W1∈R \n",
    "hidden_size×input_size\n",
    " \n",
    "\n",
    "b1∈R \n",
    "hidden_size×1\n",
    " \n",
    "\n",
    "W2∈R \n",
    "output_size×hidden_size\n",
    " \n",
    "\n",
    "b2∈R \n",
    "output_size×1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Defining Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Forward Pass Or Forward Propagation ?\n",
    "\n",
    "Forward propagation (or forward pass) refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(data , W1, b1 , W2 ,b2):\n",
    "    Z1 = np.dot(W1, data) + b1\n",
    "\n",
    "    A1 = np.tanh(Z1)\n",
    "\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "\n",
    "    return Z1,A1,Z2\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass function takes the input X, the weight matrices W1 and W2, and the bias vectors b1 and b2 as inputs.\n",
    "\n",
    "Input Layer:The input matrix X is the input layer.\n",
    "\n",
    "Hidden Layer: The activations A1 represent the hidden layer. This is the layer between the input layer and the output layer, where the non-linear transformation of the data occurs.\n",
    "The hidden layer takes the linear transformation Z1 and applies the activation function np.tanh() to introduce non-linearity.\n",
    "The hidden layer learns to extract and represent meaningful features from the input data.\n",
    "\n",
    "Output Layer: The activations Z2 represent the output layer.\n",
    "The output layer takes the activated values A1 from the hidden layer and applies another linear transformation to produce the final output Z2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Calculate the loss (Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_loss(y, y_pred):\n",
    "    m = y.shape[0]\n",
    "    loss = np.sum((y_pred - y) ** 2) / (2 * m)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Backward Propagation to compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is backward propagation ?\n",
    "\n",
    "Backward propagation (or backward pass) is the process of calculating the gradients of the loss with respect to the weights and biases for each layer in the neural network. This is crucial for updating the weights and biases to minimize the loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation (X , Y , Z1 , A1, Z2, W2):\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Output Layer Gradient\n",
    "    dZ2 = Z2 - Y\n",
    "    dW2 = np.dot(dZ2,A1.T)/m\n",
    "    db2 = np.sum(dZ2,axis=1,keepdims=True)/m\n",
    "\n",
    "    # Hidden Layer Gradient\n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    dZ1 = dA1 * (1 -  np.tanh(Z1)** 2 )\n",
    "    dW1 = np.dot(dZ1,X.T)/m\n",
    "    db1 = np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "    return dW1, db1, dW2, db2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Each Variable:\n",
    "\n",
    "dZ2 = Difference between predicted output and true output\n",
    "\n",
    "dW2 = Gradient of cost function with respect to weights in the output layer\n",
    "\n",
    "db2 = Gradient of cost function with respect to biases in the output layer\n",
    "\n",
    "dA1 = Activation function derivative of hidden layer\n",
    "\n",
    "dZ1 = Derivative of linear transformation of hidden layer\n",
    "\n",
    "dW1 = Gradient of cost function with respect to weights in the hidden layer\n",
    "\n",
    "db1 = Gradient of cost function with respect to biases in the hidden layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Update the weights using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = center> <font color = '#1128FF'>Now Merging All the above fucntions together to make a train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(X,y,input_size,hidden_size,output_size,epochs,learning_rate):\n",
    "    W1, b1, W2, b2 = parameter_initialization(input_size, hidden_size, output_size)\n",
    "    for i in range(epochs):\n",
    "        Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        loss = computing_loss(y, Z2)\n",
    "\n",
    "        dW1 , db1, dW2 , db2 = backward_propagation(X,y, Z1, A1, Z2,W2)\n",
    "\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {i}, Loss: {loss}')\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Prediction Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (X , W1 , b1, W2, b2):\n",
    "    a,b,Z2 = forward_propagation(X,W1,b1,W2,b2)\n",
    "    return Z2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "liver_disorders = fetch_ucirepo(id=60) \n",
    "\n",
    "X = liver_disorders.data.features.values.T\n",
    "y = liver_disorders.data.targets.values.T\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Using The TrainNN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 345)\n",
      "(1, 345)\n",
      "Epoch 0, Loss: 3189.134682082662\n",
      "Epoch 100, Loss: 2891.620406919029\n",
      "Epoch 200, Loss: 2648.03138641206\n",
      "Epoch 300, Loss: 2448.550457700655\n",
      "Epoch 400, Loss: 2285.129360462447\n",
      "Epoch 500, Loss: 2151.1565648462324\n",
      "Epoch 600, Loss: 2041.1842600589976\n",
      "Epoch 700, Loss: 1950.7018408070421\n",
      "Epoch 800, Loss: 1875.9475909381674\n",
      "Epoch 900, Loss: 1813.7546894210132\n",
      "Epoch 1000, Loss: 1761.4329629522247\n",
      "Epoch 1100, Loss: 1716.6924277249059\n",
      "Epoch 1200, Loss: 1677.6133871357133\n",
      "Epoch 1300, Loss: 1642.6546320468306\n",
      "Epoch 1400, Loss: 1610.67016603724\n",
      "Epoch 1500, Loss: 1580.8963440414582\n",
      "Epoch 1600, Loss: 1552.8914108531712\n",
      "Epoch 1700, Loss: 1526.4425552993528\n",
      "Epoch 1800, Loss: 1501.4716120995954\n",
      "Epoch 1900, Loss: 1477.9625375948467\n",
      "Epoch 2000, Loss: 1455.9171827069995\n",
      "Epoch 2100, Loss: 1435.334187474295\n",
      "Epoch 2200, Loss: 1416.2019291002468\n",
      "Epoch 2300, Loss: 1398.4977169372523\n",
      "Epoch 2400, Loss: 1382.1885564346944\n",
      "Epoch 2500, Loss: 1367.231651503653\n",
      "Epoch 2600, Loss: 1353.574531704228\n",
      "Epoch 2700, Loss: 1341.1553144574673\n",
      "Epoch 2800, Loss: 1329.9035566032976\n",
      "Epoch 2900, Loss: 1319.7418387829\n",
      "Epoch 3000, Loss: 1310.5879356280554\n",
      "Epoch 3100, Loss: 1302.3572675940482\n",
      "Epoch 3200, Loss: 1294.9653097200749\n",
      "Epoch 3300, Loss: 1288.3297022054217\n",
      "Epoch 3400, Loss: 1282.371913619627\n",
      "Epoch 3500, Loss: 1277.0184095058626\n",
      "Epoch 3600, Loss: 1272.2013555514718\n",
      "Epoch 3700, Loss: 1267.8589295131708\n",
      "Epoch 3800, Loss: 1263.9353335602996\n",
      "Epoch 3900, Loss: 1260.380596709314\n",
      "Epoch 4000, Loss: 1257.1502440081094\n",
      "Epoch 4100, Loss: 1254.204891875012\n",
      "Epoch 4200, Loss: 1251.5098119919326\n",
      "Epoch 4300, Loss: 1249.0344917732323\n",
      "Epoch 4400, Loss: 1246.7522085016033\n",
      "Epoch 4500, Loss: 1244.6396266029772\n",
      "Epoch 4600, Loss: 1242.6764226239345\n",
      "Epoch 4700, Loss: 1240.844939549795\n",
      "Epoch 4800, Loss: 1239.1298704943802\n",
      "Epoch 4900, Loss: 1237.5179709822555\n",
      "Epoch 5000, Loss: 1235.9977986674187\n",
      "Epoch 5100, Loss: 1234.5594791614112\n",
      "Epoch 5200, Loss: 1233.1944965530151\n",
      "Epoch 5300, Loss: 1231.8955071332384\n",
      "Epoch 5400, Loss: 1230.6561747752257\n",
      "Epoch 5500, Loss: 1229.4710263608758\n",
      "Epoch 5600, Loss: 1228.3353256030327\n",
      "Epoch 5700, Loss: 1227.2449635927287\n",
      "Epoch 5800, Loss: 1226.196364410302\n",
      "Epoch 5900, Loss: 1225.1864041784384\n",
      "Epoch 6000, Loss: 1224.212342001996\n",
      "Epoch 6100, Loss: 1223.271761329182\n",
      "Epoch 6200, Loss: 1222.3625203753388\n",
      "Epoch 6300, Loss: 1221.4827103681232\n",
      "Epoch 6400, Loss: 1220.6306204954626\n",
      "Epoch 6500, Loss: 1219.8047085604378\n",
      "Epoch 6600, Loss: 1219.003576466278\n",
      "Epoch 6700, Loss: 1218.2259497670993\n",
      "Epoch 6800, Loss: 1217.4706606239822\n",
      "Epoch 6900, Loss: 1216.7366336003786\n",
      "Epoch 7000, Loss: 1216.02287381524\n",
      "Epoch 7100, Loss: 1215.328457046744\n",
      "Epoch 7200, Loss: 1214.6525214444666\n",
      "Epoch 7300, Loss: 1213.9942605639808\n",
      "Epoch 7400, Loss: 1213.3529174859045\n",
      "Epoch 7500, Loss: 1212.727779822255\n",
      "Epoch 7600, Loss: 1212.1181754473841\n",
      "Epoch 7700, Loss: 1211.5234688196429\n",
      "Epoch 7800, Loss: 1210.9430577839469\n",
      "Epoch 7900, Loss: 1210.3763707653357\n",
      "Epoch 8000, Loss: 1209.822864280004\n",
      "Epoch 8100, Loss: 1209.2820207037134\n",
      "Epoch 8200, Loss: 1208.7533462483952\n",
      "Epoch 8300, Loss: 1208.2363691065902\n",
      "Epoch 8400, Loss: 1207.7306377304164\n",
      "Epoch 8500, Loss: 1207.2357192173447\n",
      "Epoch 8600, Loss: 1206.7511977794265\n",
      "Epoch 8700, Loss: 1206.2766732759374\n",
      "Epoch 8800, Loss: 1205.8117597918508\n",
      "Epoch 8900, Loss: 1205.3560842462657\n",
      "Epoch 9000, Loss: 1204.909285015952\n",
      "Epoch 9100, Loss: 1204.4710105596582\n",
      "Epoch 9200, Loss: 1204.0409180287334\n",
      "Epoch 9300, Loss: 1203.6186718490396\n",
      "Epoch 9400, Loss: 1203.2039422579778\n",
      "Epoch 9500, Loss: 1202.796403778756\n",
      "Epoch 9600, Loss: 1202.3957336116887\n",
      "Epoch 9700, Loss: 1202.0016099192133\n",
      "Epoch 9800, Loss: 1201.6137099773275\n",
      "Epoch 9900, Loss: 1201.23170816103\n",
      "Epoch 10000, Loss: 1200.8552737248392\n",
      "Epoch 10100, Loss: 1200.4840683311318\n",
      "Epoch 10200, Loss: 1200.1177432683944\n",
      "Epoch 10300, Loss: 1199.7559362877603\n",
      "Epoch 10400, Loss: 1199.3982679684652\n",
      "Epoch 10500, Loss: 1199.0443374997299\n",
      "Epoch 10600, Loss: 1198.6937177362875\n",
      "Epoch 10700, Loss: 1198.3459493447272\n",
      "Epoch 10800, Loss: 1198.0005338045928\n",
      "Epoch 10900, Loss: 1197.656924956762\n",
      "Epoch 11000, Loss: 1197.3145186951526\n",
      "Epoch 11100, Loss: 1196.9726402663853\n",
      "Epoch 11200, Loss: 1196.6305284615262\n",
      "Epoch 11300, Loss: 1196.287315734127\n",
      "Epoch 11400, Loss: 1195.9420029299145\n",
      "Epoch 11500, Loss: 1195.5934268226956\n",
      "Epoch 11600, Loss: 1195.2402179555934\n",
      "Epoch 11700, Loss: 1194.8807452952524\n",
      "Epoch 11800, Loss: 1194.5130427871918\n",
      "Epoch 11900, Loss: 1194.1347108662453\n",
      "Epoch 12000, Loss: 1193.7427830735387\n",
      "Epoch 12100, Loss: 1193.3335438470021\n",
      "Epoch 12200, Loss: 1192.9022779825527\n",
      "Epoch 12300, Loss: 1192.4429251698161\n",
      "Epoch 12400, Loss: 1191.9476053565686\n",
      "Epoch 12500, Loss: 1191.4059763852874\n",
      "Epoch 12600, Loss: 1190.8043957640343\n",
      "Epoch 12700, Loss: 1190.1249120767643\n",
      "Epoch 12800, Loss: 1189.3442697944238\n",
      "Epoch 12900, Loss: 1188.4334774391305\n",
      "Epoch 13000, Loss: 1187.3591422069605\n",
      "Epoch 13100, Loss: 1186.0884500344646\n",
      "Epoch 13200, Loss: 1184.5991319884388\n",
      "Epoch 13300, Loss: 1182.8921046459427\n",
      "Epoch 13400, Loss: 1180.9990960188554\n",
      "Epoch 13500, Loss: 1178.9782213371955\n",
      "Epoch 13600, Loss: 1176.8994544615707\n",
      "Epoch 13700, Loss: 1174.8274018700502\n",
      "Epoch 13800, Loss: 1172.807620056482\n",
      "Epoch 13900, Loss: 1170.862015871576\n",
      "Epoch 14000, Loss: 1168.9942013866394\n",
      "Epoch 14100, Loss: 1167.1986520438375\n",
      "Epoch 14200, Loss: 1165.4676562415852\n",
      "Epoch 14300, Loss: 1163.7946482491748\n",
      "Epoch 14400, Loss: 1162.1751022035205\n",
      "Epoch 14500, Loss: 1160.6063000826134\n",
      "Epoch 14600, Loss: 1159.0867463679858\n",
      "Epoch 14700, Loss: 1157.615592606553\n",
      "Epoch 14800, Loss: 1156.1922133049418\n",
      "Epoch 14900, Loss: 1154.8159567126088\n",
      "Epoch 15000, Loss: 1153.4860353370223\n",
      "Epoch 15100, Loss: 1152.2015036230678\n",
      "Epoch 15200, Loss: 1150.9612779663378\n",
      "Epoch 15300, Loss: 1149.7641706735967\n",
      "Epoch 15400, Loss: 1148.6089239151154\n",
      "Epoch 15500, Loss: 1147.4942386325797\n",
      "Epoch 15600, Loss: 1146.4187975923176\n",
      "Epoch 15700, Loss: 1145.3812832628714\n",
      "Epoch 15800, Loss: 1144.3803914717141\n",
      "Epoch 15900, Loss: 1143.4148416758908\n",
      "Epoch 16000, Loss: 1142.483384498909\n",
      "Epoch 16100, Loss: 1141.584807038653\n",
      "Epoch 16200, Loss: 1140.7179363486086\n",
      "Epoch 16300, Loss: 1139.8816414251305\n",
      "Epoch 16400, Loss: 1139.0748339843512\n",
      "Epoch 16500, Loss: 1138.2964682749503\n",
      "Epoch 16600, Loss: 1137.5455401423014\n",
      "Epoch 16700, Loss: 1136.8210855326593\n",
      "Epoch 16800, Loss: 1136.122178601462\n",
      "Epoch 16900, Loss: 1135.4479295668248\n",
      "Epoch 17000, Loss: 1134.7974824275307\n",
      "Epoch 17100, Loss: 1134.1700126442797\n",
      "Epoch 17200, Loss: 1133.5647248637047\n",
      "Epoch 17300, Loss: 1132.9808507468804\n",
      "Epoch 17400, Loss: 1132.4176469479053\n",
      "Epoch 17500, Loss: 1131.8743932737216\n",
      "Epoch 17600, Loss: 1131.3503910438128\n",
      "Epoch 17700, Loss: 1130.8449616577602\n",
      "Epoch 17800, Loss: 1130.357445369843\n",
      "Epoch 17900, Loss: 1129.8872002628807\n",
      "Epoch 18000, Loss: 1129.4336014081834\n",
      "Epoch 18100, Loss: 1128.9960401946748\n",
      "Epoch 18200, Loss: 1128.5739238077979\n",
      "Epoch 18300, Loss: 1128.1666748375167\n",
      "Epoch 18400, Loss: 1127.773730994386\n",
      "Epoch 18500, Loss: 1127.3945449131184\n",
      "Epoch 18600, Loss: 1127.0285840241177\n",
      "Epoch 18700, Loss: 1126.6753304749386\n",
      "Epoch 18800, Loss: 1126.3342810854226\n",
      "Epoch 18900, Loss: 1126.0049473222061\n",
      "Epoch 19000, Loss: 1125.6868552803462\n",
      "Epoch 19100, Loss: 1125.3795456618095\n",
      "Epoch 19200, Loss: 1125.082573742528\n",
      "Epoch 19300, Loss: 1124.7955093215369\n",
      "Epoch 19400, Loss: 1124.5179366473806\n",
      "Epoch 19500, Loss: 1124.2494543184541\n",
      "Epoch 19600, Loss: 1123.9896751552692\n",
      "Epoch 19700, Loss: 1123.7382260437278\n",
      "Epoch 19800, Loss: 1123.4947477494525\n",
      "Epoch 19900, Loss: 1123.2588947039644\n",
      "Epoch 20000, Loss: 1123.0303347641288\n",
      "Epoch 20100, Loss: 1122.8087489467425\n",
      "Epoch 20200, Loss: 1122.5938311404798\n",
      "Epoch 20300, Loss: 1122.3852877976506\n",
      "Epoch 20400, Loss: 1122.182837608351\n",
      "Epoch 20500, Loss: 1121.9862111596442\n",
      "Epoch 20600, Loss: 1121.795150582408\n",
      "Epoch 20700, Loss: 1121.6094091884177\n",
      "Epoch 20800, Loss: 1121.4287511001316\n",
      "Epoch 20900, Loss: 1121.2529508755206\n",
      "Epoch 21000, Loss: 1121.0817931301206\n",
      "Epoch 21100, Loss: 1120.9150721583246\n",
      "Epoch 21200, Loss: 1120.7525915557474\n",
      "Epoch 21300, Loss: 1120.5941638443237\n",
      "Epoch 21400, Loss: 1120.4396101016098\n",
      "Epoch 21500, Loss: 1120.2887595955908\n",
      "Epoch 21600, Loss: 1120.141449426124\n",
      "Epoch 21700, Loss: 1119.9975241739853\n",
      "Epoch 21800, Loss: 1119.8568355583352\n",
      "Epoch 21900, Loss: 1119.7192421032823\n",
      "Epoch 22000, Loss: 1119.5846088140856\n",
      "Epoch 22100, Loss: 1119.4528068634286\n",
      "Epoch 22200, Loss: 1119.323713288075\n",
      "Epoch 22300, Loss: 1119.1972106961332\n",
      "Epoch 22400, Loss: 1119.0731869850642\n",
      "Epoch 22500, Loss: 1118.9515350704846\n",
      "Epoch 22600, Loss: 1118.8321526257614\n",
      "Epoch 22700, Loss: 1118.7149418323258\n",
      "Epoch 22800, Loss: 1118.5998091405872\n",
      "Epoch 22900, Loss: 1118.4866650412846\n",
      "Epoch 23000, Loss: 1118.375423847079\n",
      "Epoch 23100, Loss: 1118.2660034841517\n",
      "Epoch 23200, Loss: 1118.1583252935598\n",
      "Epoch 23300, Loss: 1118.0523138420724\n",
      "Epoch 23400, Loss: 1117.9478967421983\n",
      "Epoch 23500, Loss: 1117.8450044811066\n",
      "Epoch 23600, Loss: 1117.743570258131\n",
      "Epoch 23700, Loss: 1117.6435298305441\n",
      "Epoch 23800, Loss: 1117.5448213672887\n",
      "Epoch 23900, Loss: 1117.447385310352\n",
      "Epoch 24000, Loss: 1117.3511642434682\n",
      "Epoch 24100, Loss: 1117.2561027678428\n",
      "Epoch 24200, Loss: 1117.1621473845953\n",
      "Epoch 24300, Loss: 1117.0692463836253\n",
      "Epoch 24400, Loss: 1116.9773497386107\n",
      "Epoch 24500, Loss: 1116.8864090078596\n",
      "Epoch 24600, Loss: 1116.7963772407443\n",
      "Epoch 24700, Loss: 1116.7072088894543\n",
      "Epoch 24800, Loss: 1116.6188597258172\n",
      "Epoch 24900, Loss: 1116.531286762945\n",
      "Epoch 25000, Loss: 1116.4444481814721\n",
      "Epoch 25100, Loss: 1116.3583032601664\n",
      "Epoch 25200, Loss: 1116.272812310699\n",
      "Epoch 25300, Loss: 1116.1879366163723\n",
      "Epoch 25400, Loss: 1116.1036383746139\n",
      "Epoch 25500, Loss: 1116.0198806430549\n",
      "Epoch 25600, Loss: 1115.9366272890234\n",
      "Epoch 25700, Loss: 1115.8538429422836\n",
      "Epoch 25800, Loss: 1115.7714929508725\n",
      "Epoch 25900, Loss: 1115.6895433398895\n",
      "Epoch 26000, Loss: 1115.6079607730956\n",
      "Epoch 26100, Loss: 1115.5267125172036\n",
      "Epoch 26200, Loss: 1115.44576640873\n",
      "Epoch 26300, Loss: 1115.3650908233003\n",
      "Epoch 26400, Loss: 1115.2846546473022\n",
      "Epoch 26500, Loss: 1115.2044272517833\n",
      "Epoch 26600, Loss: 1115.1243784685053\n",
      "Epoch 26700, Loss: 1115.0444785680638\n",
      "Epoch 26800, Loss: 1114.9646982399995\n",
      "Epoch 26900, Loss: 1114.8850085748136\n",
      "Epoch 27000, Loss: 1114.805381047831\n",
      "Epoch 27100, Loss: 1114.7257875048324\n",
      "Epoch 27200, Loss: 1114.6462001494042\n",
      "Epoch 27300, Loss: 1114.566591531941\n",
      "Epoch 27400, Loss: 1114.486934540249\n",
      "Epoch 27500, Loss: 1114.4072023917029\n",
      "Epoch 27600, Loss: 1114.3273686269026\n",
      "Epoch 27700, Loss: 1114.2474071047889\n",
      "Epoch 27800, Loss: 1114.1672919991724\n",
      "Epoch 27900, Loss: 1114.0869977966363\n",
      "Epoch 28000, Loss: 1114.0064992957723\n",
      "Epoch 28100, Loss: 1113.9257716077118\n",
      "Epoch 28200, Loss: 1113.844790157915\n",
      "Epoch 28300, Loss: 1113.7635306891816\n",
      "Epoch 28400, Loss: 1113.6819692658453\n",
      "Epoch 28500, Loss: 1113.600082279116\n",
      "Epoch 28600, Loss: 1113.5178464535359\n",
      "Epoch 28700, Loss: 1113.4352388545085\n",
      "Epoch 28800, Loss: 1113.3522368968652\n",
      "Epoch 28900, Loss: 1113.268818354425\n",
      "Epoch 29000, Loss: 1113.184961370519\n",
      "Epoch 29100, Loss: 1113.100644469422\n",
      "Epoch 29200, Loss: 1113.015846568661\n",
      "Epoch 29300, Loss: 1112.9305469921478\n",
      "Epoch 29400, Loss: 1112.8447254840878\n",
      "Epoch 29500, Loss: 1112.7583622236189\n",
      "Epoch 29600, Loss: 1112.6714378401264\n",
      "Epoch 29700, Loss: 1112.5839334291766\n",
      "Epoch 29800, Loss: 1112.4958305690145\n",
      "Epoch 29900, Loss: 1112.4071113375644\n",
      "Epoch 30000, Loss: 1112.3177583298707\n",
      "Epoch 30100, Loss: 1112.2277546759124\n",
      "Epoch 30200, Loss: 1112.1370840587233\n",
      "Epoch 30300, Loss: 1112.0457307327504\n",
      "Epoch 30400, Loss: 1111.953679542371\n",
      "Epoch 30500, Loss: 1111.8609159404991\n",
      "Epoch 30600, Loss: 1111.7674260072\n",
      "Epoch 30700, Loss: 1111.6731964682313\n",
      "Epoch 30800, Loss: 1111.5782147134396\n",
      "Epoch 30900, Loss: 1111.482468814912\n",
      "Epoch 31000, Loss: 1111.3859475448173\n",
      "Epoch 31100, Loss: 1111.288640392838\n",
      "Epoch 31200, Loss: 1111.190537583116\n",
      "Epoch 31300, Loss: 1111.0916300906235\n",
      "Epoch 31400, Loss: 1110.9919096568733\n",
      "Epoch 31500, Loss: 1110.8913688048863\n",
      "Epoch 31600, Loss: 1110.790000853333\n",
      "Epoch 31700, Loss: 1110.6877999297649\n",
      "Epoch 31800, Loss: 1110.5847609828604\n",
      "Epoch 31900, Loss: 1110.4808797936043\n",
      "Epoch 32000, Loss: 1110.3761529853332\n",
      "Epoch 32100, Loss: 1110.2705780325673\n",
      "Epoch 32200, Loss: 1110.164153268574\n",
      "Epoch 32300, Loss: 1110.0568778915922\n",
      "Epoch 32400, Loss: 1109.9487519696665\n",
      "Epoch 32500, Loss: 1109.8397764440344\n",
      "Epoch 32600, Loss: 1109.729953131027\n",
      "Epoch 32700, Loss: 1109.6192847224397\n",
      "Epoch 32800, Loss: 1109.507774784336\n",
      "Epoch 32900, Loss: 1109.395427754267\n",
      "Epoch 33000, Loss: 1109.2822489368762\n",
      "Epoch 33100, Loss: 1109.1682444978837\n",
      "Epoch 33200, Loss: 1109.0534214564404\n",
      "Epoch 33300, Loss: 1108.937787675855\n",
      "Epoch 33400, Loss: 1108.8213518526995\n",
      "Epoch 33500, Loss: 1108.7041235043162\n",
      "Epoch 33600, Loss: 1108.586112954738\n",
      "Epoch 33700, Loss: 1108.4673313190651\n",
      "Epoch 33800, Loss: 1108.3477904863244\n",
      "Epoch 33900, Loss: 1108.2275031008612\n",
      "Epoch 34000, Loss: 1108.1064825423114\n",
      "Epoch 34100, Loss: 1107.9847429042125\n",
      "Epoch 34200, Loss: 1107.8622989713115\n",
      "Epoch 34300, Loss: 1107.7391661956442\n",
      "Epoch 34400, Loss: 1107.6153606714497\n",
      "Epoch 34500, Loss: 1107.4908991090053\n",
      "Epoch 34600, Loss: 1107.3657988074572\n",
      "Epoch 34700, Loss: 1107.240077626736\n",
      "Epoch 34800, Loss: 1107.113753958641\n",
      "Epoch 34900, Loss: 1106.986846697188\n",
      "Epoch 35000, Loss: 1106.8593752083082\n",
      "Epoch 35100, Loss: 1106.7313592989951\n",
      "Epoch 35200, Loss: 1106.602819185995\n",
      "Epoch 35300, Loss: 1106.4737754641326\n",
      "Epoch 35400, Loss: 1106.34424907437\n",
      "Epoch 35500, Loss: 1106.2142612716932\n",
      "Epoch 35600, Loss: 1106.0838335929175\n",
      "Epoch 35700, Loss: 1105.9529878245066\n",
      "Epoch 35800, Loss: 1105.8217459704924\n",
      "Epoch 35900, Loss: 1105.6901302205847\n",
      "Epoch 36000, Loss: 1105.5581629185551\n",
      "Epoch 36100, Loss: 1105.4258665309776\n",
      "Epoch 36200, Loss: 1105.2932636164019\n",
      "Epoch 36300, Loss: 1105.1603767950355\n",
      "Epoch 36400, Loss: 1105.0272287190028\n",
      "Epoch 36500, Loss: 1104.8938420432537\n",
      "Epoch 36600, Loss: 1104.7602393971733\n",
      "Epoch 36700, Loss: 1104.6264433569586\n",
      "Epoch 36800, Loss: 1104.4924764188127\n",
      "Epoch 36900, Loss: 1104.3583609729997\n",
      "Epoch 37000, Loss: 1104.2241192788124\n",
      "Epoch 37100, Loss: 1104.0897734404816\n",
      "Epoch 37200, Loss: 1103.9553453840658\n",
      "Epoch 37300, Loss: 1103.8208568353518\n",
      "Epoch 37400, Loss: 1103.686329298784\n",
      "Epoch 37500, Loss: 1103.551784037448\n",
      "Epoch 37600, Loss: 1103.4172420541197\n",
      "Epoch 37700, Loss: 1103.2827240733934\n",
      "Epoch 37800, Loss: 1103.148250524891\n",
      "Epoch 37900, Loss: 1103.0138415275608\n",
      "Epoch 38000, Loss: 1102.879516875059\n",
      "Epoch 38100, Loss: 1102.7452960222104\n",
      "Epoch 38200, Loss: 1102.611198072543\n",
      "Epoch 38300, Loss: 1102.4772417668767\n",
      "Epoch 38400, Loss: 1102.3434454729613\n",
      "Epoch 38500, Loss: 1102.209827176137\n",
      "Epoch 38600, Loss: 1102.0764044710022\n",
      "Epoch 38700, Loss: 1101.943194554065\n",
      "Epoch 38800, Loss: 1101.810214217349\n",
      "Epoch 38900, Loss: 1101.6774798429371\n",
      "Epoch 39000, Loss: 1101.5450073984105\n",
      "Epoch 39100, Loss: 1101.4128124331642\n",
      "Epoch 39200, Loss: 1101.2809100755621\n",
      "Epoch 39300, Loss: 1101.149315030902\n",
      "Epoch 39400, Loss: 1101.018041580152\n",
      "Epoch 39500, Loss: 1100.887103579431\n",
      "Epoch 39600, Loss: 1100.7565144601954\n",
      "Epoch 39700, Loss: 1100.626287230093\n",
      "Epoch 39800, Loss: 1100.4964344744587\n",
      "Epoch 39900, Loss: 1100.3669683584046\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Splitting Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size=0.2, random_state=42)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.T).T\n",
    "X_test = scaler.transform(X_test.T).T\n",
    "\n",
    "\n",
    "\n",
    "# Setting Up Hyperparameters For Neural Network\n",
    "input_size = X_train.shape[0]\n",
    "hidden_size = 2\n",
    "output_size = y_train.shape[0]\n",
    "epochs = 40000\n",
    "learning_rate = 0.001\n",
    "\n",
    "W1, b1, W2, b2 = trainNN(X_train, y_train, input_size, hidden_size, output_size, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.142097812255336\n",
      "Mean Absolute Error: 2.4152657013875922\n",
      "R-squared: 0.14116106741093226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = predict(X_test, W1, b1, W2, b2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_test_flat = y_test.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "r2 = r2_score(y_test_flat, y_pred_flat)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = center> <font color = \"#FFA009\"> Conclusion </font></h2>\n",
    "\n",
    "The model evaluation shows that the model score is 14% which can be further improved by setting the hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
